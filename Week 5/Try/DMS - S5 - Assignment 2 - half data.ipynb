{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this is a template notebook for Assignment 2 on Clustering. To get a 60 you will need to complete chapter 1 and 2.\n",
    "    The template is also just an indication. You can add more cells if needed, and can of course delete this line**\n",
    "\n",
    "# <span>Classification Assignment: Australia Weather<Title of your notebook>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <span>Stephen Pangga </span><br>\n",
    "Student number: <span> 629860 </span><br>\n",
    "Date: <span> 21/05/2022 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work you must have installed the following packages (usually via pip install *packageName*:\n",
    "* numpy\n",
    "* pandas\n",
    "* **\\<add other packages\\>**\n",
    "\n",
    "From these we will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy already installed, only imported\n",
      "PyPlot already installed, only imported\n",
      "pandas already installed, only imported\n",
      "seaborn already installed, only imported\n",
      "sklearn already installed, only imported\n"
     ]
    }
   ],
   "source": [
    "# enter here all those 'from .... import ....'\n",
    "# numpy as np\n",
    "try:\n",
    "    import numpy as np\n",
    "    print('NumPy already installed, only imported')\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    print('NumPy was not installed, installed and imported')\n",
    "      \n",
    "# pyplot as plt\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot already installed, only imported')\n",
    "except:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot was not installed, installed and imported')\n",
    "\n",
    "# pandas as pd   \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print('pandas already installed, only imported')\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    print('pandas was not installed, installed and imported')\n",
    "    \n",
    "try:\n",
    "    import seaborn as sn\n",
    "    print('seaborn already installed, only imported')\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sn\n",
    "    print('seaborn was not installed, installed and imported')\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print('sklearn already installed, only imported')\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print('sklearn was not installed, installed and imported')\n",
    "    \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Data\n",
    "We are going to use the datafile **<span style ='background:yellow'>weatherAUS.csv</span>**. This contains data **<span style ='background:yellow'> from kaggle and its about the daily weather for the past 10 years from the country Australia </span>**.\n",
    "\n",
    "\n",
    "\n",
    "The explanation of the variable and type from https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the code to load the data\n",
    "data = pd.read_csv('weatherAUSv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>17.9</td>\n",
       "      <td>35.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SSW</td>\n",
       "      <td>48.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>1004.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>33.4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>S</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1012.9</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>19.4</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>NNE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>21.9</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>WNW</td>\n",
       "      <td>31.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>1009.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>24.2</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>WNW</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "6049  2009-01-01    Cobar     17.9     35.2       0.0         12.0      12.3   \n",
       "6050  2009-01-02    Cobar     18.4     28.9       0.0         14.8      13.0   \n",
       "6052  2009-01-04    Cobar     19.4     37.6       0.0         10.8      10.6   \n",
       "6053  2009-01-05    Cobar     21.9     38.4       0.0         11.4      12.2   \n",
       "6054  2009-01-06    Cobar     24.2     41.0       0.0         11.2       8.4   \n",
       "\n",
       "     WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "6049         SSW           48.0        ENE  ...        20.0         13.0   \n",
       "6050           S           37.0        SSE  ...        30.0          8.0   \n",
       "6052         NNE           46.0        NNE  ...        42.0         22.0   \n",
       "6053         WNW           31.0        WNW  ...        37.0         22.0   \n",
       "6054         WNW           35.0         NW  ...        19.0         15.0   \n",
       "\n",
       "      Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  \\\n",
       "6049       1006.3       1004.4       2.0       5.0     26.6     33.4   \n",
       "6050       1012.9       1012.1       1.0       1.0     20.3     27.0   \n",
       "6052       1012.3       1009.2       1.0       6.0     28.7     34.9   \n",
       "6053       1012.7       1009.1       1.0       5.0     29.1     35.6   \n",
       "6054       1010.7       1007.4       1.0       6.0     33.6     37.6   \n",
       "\n",
       "      RainToday  RainTomorrow  \n",
       "6049         No            No  \n",
       "6050         No            No  \n",
       "6052         No            No  \n",
       "6053         No            No  \n",
       "6054         No            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter the code to see the first few rows of the data\n",
    "#remove na values\n",
    "data.dropna(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the variables (/fields/columns) you are going to use for your prediction. The dependent y (the one you are going to predict) and two independent x's (the ones you are going to use to predict that y). What do these variables mean?\\></span>**\n",
    "*note:* Its easiest if your independent x variables are numeric.\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The variabes that ill be using are Rainfall, RainToday and RainTomorrow are going to be use for my prediction.\n",
    "The Y dependent will be the RainTomorrow, the X indepentedent variables that i will be using are RainFall and RainToday.\n",
    "The x variables:\n",
    "* Rainfall - is the amount of rainfall that was recorded for the day. Measured in mm.\n",
    "* RainToday - is the representation if it rain today or not. The data is a boolean, 1 (yes) if the precipitation in the last 24 hours has exceeded 1mm if not then 0 (no).\n",
    "The y variables:\n",
    "* RainTomorrow: is the amount of rain the next day in mm. A kind of measurement or the posibility of rain the next day presented in boolean values.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical dependent variable <span style ='background:blue'> RainTomorrow <your y variable name></span> has the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['No', 'Yes'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code that returns the different categories in the y variable.\n",
    "data['RainTomorrow'] = pd.Categorical(data['RainTomorrow'])\n",
    "data['RainTomorrow'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rainfall  RainToday  RainTomorrow\n",
       "6049       0.0          0             0\n",
       "6050       0.0          0             0\n",
       "6052       0.0          0             0\n",
       "6053       0.0          0             0\n",
       "6054       0.0          0             0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert yes or no to 1 or 2 as it will be easier to handle\n",
    "data['RainToday'] = pd.Categorical(data['RainToday'])\n",
    "\n",
    "#to make it easier to visualize the data make a new dataframe with the column i will be using\n",
    "weather_df = pd.DataFrame()\n",
    "\n",
    "#add to the dataframe\n",
    "weather_df['Rainfall'] = data['Rainfall']\n",
    "weather_df['RainToday'] = data['RainToday'].cat.codes\n",
    "weather_df['RainTomorrow'] = data['RainTomorrow'].cat.codes\n",
    "\n",
    "#a small visualization of the new dataframe\n",
    "weather_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some training and testing data, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split data in training and testing\n",
    "\n",
    "#set the variables\n",
    "x_var = weather_df[['Rainfall', 'RainToday']]\n",
    "y_var = weather_df['RainTomorrow']\n",
    "\n",
    "#split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.4, random_state=101)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set, lets try to predict this using our independent variables **<span> Rainfall & RainToday.<your x variables names></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Basic Classification Models\n",
    "\n",
    "In the Jupyter Notebook from lecture 5 a few different Clustering techniques were discussed. Lets explore how these perform on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at some 'real' models, its a good idea to get a baseline in by using one or more of the dummy classifiers. Lets see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classsifier - Most Frequent\n",
      "The accuracy score:  0.7632679318108717\n",
      "[[4746    0]\n",
      " [1472    0]]\n",
      "Dummy classsifier - Uniform\n",
      "The accuracy score:  0.4975876487616597\n",
      "[[2393 2353]\n",
      " [ 771  701]]\n",
      "Dummy classsifier - Stratified\n",
      "The accuracy score:  0.6362174332582824\n",
      "[[3638 1108]\n",
      " [1154  318]]\n",
      "Dummy classsifier - Prior\n",
      "The accuracy score:  0.7632679318108717\n",
      "[[4746    0]\n",
      " [1472    0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create, fit and measure the dummy classifiers (see chapter 5.4. in the lecture notebook)\n",
    "# include both the accuracy score and the confusion matrix for each.\n",
    "\n",
    "# print(\"Dummy classsifier - Most Frequent\")\n",
    "# dumMF = DummyClassifier(strategy='most_frequent')\n",
    "# dumMF = dumMF.fit(x_train, y_train)\n",
    "# y_pred = dumMF.predict(x_test)\n",
    "\n",
    "def dummy_classifier (xtrain, xtest, ytrain, strats):\n",
    "    dumMF = DummyClassifier(strategy=strats)\n",
    "    dumMF = dumMF.fit(xtrain, ytrain)\n",
    "    y_pred = dumMF.predict(xtest)\n",
    "    print('The accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Dummy classsifier - Most Frequent\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'most_frequent')\n",
    "\n",
    "print(\"Dummy classsifier - Uniform\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'uniform')\n",
    "\n",
    "print(\"Dummy classsifier - Stratified\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'stratified')\n",
    "\n",
    "print(\"Dummy classsifier - Prior\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'prior')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The number represents how each model of the different model of classifiers performs, which shows how accurate the classifiers are and below the accuracy score is the confusion matrix to display the performance of the classifiers and summarize the performance.\n",
    "Looking at the results, 'Most frequent' & 'Prior' classifier model have performed the best scoring identical percentage of 77%.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, those are our 'baseline'. A model should be able to at least outperform these.\n",
    "\n",
    "Lets dive in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Naive Bayes\n",
    "\n",
    "The first model discussed was the Naive Bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a Naive Bayes method works></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "Naive Bayes is a algorithm that is used for classification, applying the Bayes theorem assuming that variables are independent of each other.\n",
    "Calculating probability:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}\n",
    "\\end{equation*}\n",
    "\n",
    "Using equation, we can find the probability of A based on the probability of B.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create and fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "catNB = CategoricalNB()\n",
    "catNB.fit(x_train, y_train)\n",
    "prediction1 = catNB.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7376970086844644\n",
      "[[3960  786]\n",
      " [ 845  627]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(catNB.score(x_test, y_test))\n",
    "print(confusion_matrix(y_test, prediction1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also have a look at what a prediction would be. If the **<span style ='background:yellow'>\\<your first x variable><span>** has a score of **<span style ='background:yellow'>\\<enter some value></span>** and the **<span style ='background:yellow'>\\<your other x variable></span>** has a score of **<span style ='background:yellow'>\\<enter some value></span>**, then this model will predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# code to show the prediction\n",
    "# 2009-03-02,Cobar,20,31.1,0,10.4,3,SE,30,E,NW,7,6,31,31,1013.4,1012.7,7,6,23.1,29.3,No,No\n",
    "\n",
    "# lets find a random columm for our data\n",
    "predict_test = weather_df\n",
    "predict_test.loc[0] = [4.8, 1,1]\n",
    "\n",
    "\n",
    "predictions =predict_test[[\"Rainfall\", \"RainToday\"]]\n",
    "print(catNB.predict(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about it for NB. A nice thing about NB is that it doesn't really require any parameters. Lets look at our next technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines\n",
    "The second model discussed were Support Vector Machines. There is a plural here, because we can use different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a SVM method works></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic kernel is the linear one, so we'll attempt that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svmLin = SVC(kernel = 'linear')\n",
    "svmLin.fit(x_train, y_train)\n",
    "y_pred = svmLin.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring its performance...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7701833386941138\n",
      "[[4679   67]\n",
      " [1362  110]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, and the NB?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for the other kernels that were discussed, i.e. rbf, polynomial, and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "\n",
    "#method to create model of support vector machine and show result\n",
    "def SVM (xtrain, ytrain, xtest, ytest, kernel_type):\n",
    "    svm = SVC(kernel = kernel_type)\n",
    "    svm.fit(xtrain,ytrain)\n",
    "    y_pred = svm.predict(xtest)\n",
    "    print(metrics.accuracy_score(ytest, y_pred))\n",
    "    print(confusion_matrix(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM - RBF \n",
      "0.7748472177549051\n",
      "[[4594  152]\n",
      " [1248  224]]\n"
     ]
    }
   ],
   "source": [
    "print(' SVM - RBF ')\n",
    "SVM(x_train, y_train, x_test, y_test, 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM - Sigmoid \n",
      "0.7751688645866838\n",
      "[[4540  206]\n",
      " [1192  280]]\n"
     ]
    }
   ],
   "source": [
    "print(' SVM - Sigmoid ')\n",
    "SVM(x_train, y_train, x_test, y_test, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' SVM - Polynomial ')\n",
    "# SVM(x_train, y_train, x_test, y_test, 'poly')\n",
    "\n",
    "#this SVM method took so long to give result, i ran it over night\n",
    "# 0.77717391304343783   - 0.7853260869565217\n",
    "# [[271  1]   - [[275  3]\n",
    "#  [ 81 15]]  - [76  14]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, lets move on to the third technique..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. K-Nearest Neighbors\n",
    "The third technique is the K-Nearest Neighbors (KNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a KNN method works></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we need to do some additional steps.\n",
    "\n",
    "First we need to normalize our x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the StandardScaler to normalize the two x variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#set the scalar\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# Convert the train and test X values, using the same scaler (so based on the X_train)\n",
    "X_trainScaled = scaler.transform(x_train)\n",
    "X_testScaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we need to determine how many neighbors (k) we want. To do this we'll visualize the results using different values for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the graph with Error Rate vs. K-values.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate=[]\n",
    "for i in range(1,80):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_trainScaled, y_train)\n",
    "    pred_i = knn.predict(X_testScaled)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The visualisation\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,80),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the result and your choice for k based on the graph></span>**\n",
    "\n",
    "*Note:* +0.5 if you also use the GridSearch technique to decide on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 1\n",
      "Best p: 1\n",
      "Best n_neighbors: 21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_trainScaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "\n",
    "# I ran this code and took a long time, therefore i will just show the result here again.\n",
    "\n",
    "# Best leaf_size : 1\n",
    "# Best p: 1\n",
    "# Best n_neighbors: 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch Technique took sometime to give results, i had to run it all night. So, to make it easier, I commented out the code and give the result here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want k to be, we can create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7732389835960116\n",
      "[[4560  186]\n",
      " [1224  248]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the model with the selected k\n",
    "knnOptimal = KNeighborsClassifier(n_neighbors=21, leaf_size=1, p=1)\n",
    "y_pred_optimal = knn.predict(X_testScaled)\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred_optimal))\n",
    "print(confusion_matrix(y_test, y_pred_optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7717915728530074\n"
     ]
    }
   ],
   "source": [
    "# code to create the model with the selected k\n",
    "knn = KNeighborsClassifier(n_neighbors=43)\n",
    "\n",
    "knn.fit(X_trainScaled, y_train)\n",
    "y_pred=knn.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how good it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7717915728530074\n",
      "[[4558  188]\n",
      " [1231  241]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, and the SVM kernels?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more basic technique to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Decision Trees\n",
    "The last technique that was discussed in detail, were the Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a Decision Tree method works></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variations were discussed:\n",
    "\n",
    "* ID3 (or entropy with sklearn)\n",
    "* Gini\n",
    "* Random Forest\n",
    "* Extra trees\n",
    "\n",
    "Hopefully we have the hang of this now, so lets do each of them in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - ID3/Entropy\n",
      "0.763911225474429\n",
      "[[4582  164]\n",
      " [1304  168]]\n",
      "Decision Tree - Gini\n",
      "0.763911225474429\n",
      "[[4582  164]\n",
      " [1304  168]]\n",
      "Random Forest\n",
      "0.7648761659697652\n",
      "[[4552  194]\n",
      " [1268  204]]\n",
      "Extra Trees\n",
      "0.7648761659697652\n",
      "[[4582  164]\n",
      " [1298  174]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "print('Decision Tree - ID3/Entropy')\n",
    "ent_dtc = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "ent_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = ent_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Decision Tree - Gini')\n",
    "\n",
    "gini_dtc = DecisionTreeClassifier(criterion = \"gini\")\n",
    "gini_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = gini_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Random Forest')\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfcModel = rfc.fit(X_trainScaled, y_train)\n",
    "y_pred = rfcModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Extra Trees')\n",
    "# Extremly Random Forest (a.k.a. Extra trees)\n",
    "erfc = ExtraTreesClassifier(random_state=0)\n",
    "erfc = erfc.fit(X_trainScaled, y_train)\n",
    "y_pred = erfc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, the SVM kernels, and the knn?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last set of techniques to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Other Models\n",
    "In the Jupyter Notebook from the lecture, in chapter 5.4 a few more techniques were simply shown:\n",
    "\n",
    "* Linear Discriminant Analysis\n",
    "* Quadratic Discriminant Analysis\n",
    "* Logistic Regression Classifier\n",
    "* Multinomial Logistic Regression Classification\n",
    "* Adaptive Boosting\n",
    "* Gradient Boosting\n",
    "* Histogram Gradient Boosting\n",
    "* XGBoost\n",
    "* Stacking\n",
    "\n",
    "Out of curiousity lets see how these perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis\n",
      "0.7753296880025732\n",
      "[[4580  166]\n",
      " [1231  241]]\n",
      "Quadratic Discriminant Analysis\n",
      "0.7753296880025732\n",
      "[[4580  166]\n",
      " [1231  241]]\n",
      "Logistic Regression Classifier\n",
      "0.7737214538436796\n",
      "[[4644  102]\n",
      " [1305  167]]\n",
      "Multinomial Logistic Regression Classification\n",
      "0.773882277259569\n",
      "[[4643  103]\n",
      " [1303  169]]\n",
      "Adaptive Boosting\n",
      "0.7742039240913478\n",
      "[[4539  207]\n",
      " [1197  275]]\n",
      "Gradient Boosting\n",
      "0.7685751045352204\n",
      "[[4593  153]\n",
      " [1286  186]]\n",
      "Histogram Gradient Boosting\n",
      "0.7664844001286587\n",
      "[[4548  198]\n",
      " [1254  218]]\n",
      "Stacking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7729173367642329\n",
      "[[4630  116]\n",
      " [1296  176]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score (the confusion matrix is here optional).\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "print('Linear Discriminant Analysis')\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "ldaModel=lda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "print('Quadratic Discriminant Analysis')\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qdaModel=qda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Logistic Regression Classifier')\n",
    "logreg = LogisticRegression()\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Multinomial Logistic Regression Classification')\n",
    "logreg = LogisticRegression(multi_class='multinomial')\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "print('Adaptive Boosting')\n",
    "adaBst = AdaBoostClassifier(random_state=0)\n",
    "adaBst = adaBst.fit(X_trainScaled, y_train)\n",
    "y_pred = adaBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print('Gradient Boosting')\n",
    "gradBst = GradientBoostingClassifier(random_state=0)\n",
    "gradBst = gradBst.fit(X_trainScaled, y_train)\n",
    "y_pred = gradBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "print('Histogram Gradient Boosting')\n",
    "histBst = HistGradientBoostingClassifier(random_state=0)\n",
    "histBst = histBst.fit(X_trainScaled, y_train)\n",
    "y_pred = histBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=42)))]\n",
    "\n",
    "print('Stacking')\n",
    "stackCl = StackingClassifier(estimators=estimators, final_estimator = LogisticRegression())\n",
    "stackCl.fit(X_trainScaled, y_train)\n",
    "y_pred = stackCl.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "#need to figure out xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<which performed best?></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center>-----Chapters 1 and 2 are required to be fully completed to get a 60, the next few chapters will give a +10 for each chapter.<br> \n",
    "    However the template is not as extensive as the previous chapters. <br>\n",
    "    You can select any chapter below the order is not fixed (you can leave the others empty)<br>\n",
    "    You don't have to use the same dataset for the chapters below. If it helps in clarification you can use another dataset, but then make sure to include it as you submit.\n",
    "    ----</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualisation\n",
    "\n",
    "With two input parameters we can actually determine visually where a model will classify a variable into which category. An overview of such plots is shown at https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "We cannot copy that code since it does a comparison. What we want is a function that takes the X and Y data as input, as well as the model to be used and then shows the decision areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Categorical Input\n",
    "With classification we have a categorical output variable, but what if we also have one or more categorical input variables.\n",
    "\n",
    "One popular technique is one-hot-encoding, but there are others.\n",
    "\n",
    "In this chapter we'll discuss **<span style ='background:yellow'>\\<your chosen technique></span>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the technique in detail. What does it do and how does it work></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code of using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance\n",
    "Some models get higher accuracy scores than others. In the Jupyter Notebook from the lecture the UFC data was used and the QDA had the highest accuracy score: 0.6747. The big question is, can it be done better? First areas to look for improvement are to simply increase the number of input variables, or tweak some parameters of some of the models, or a combination of both.\n",
    "\n",
    "In this chapter we'll give it an attempt.\n",
    "\n",
    "First we need to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load the UFC data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain your attempt, what did you do.></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that will generate an accuracy score for the outcome that is higher than 0.6747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A New Technique\n",
    "\n",
    "Many techniques were discussed in class and the lecture Jupyter Notebook, but there are a lot more. In this chapter the \\<your chosen new technique> is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain in detail this new technique. Note that other students should be able to understand it from your explanation alone!></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code on using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<feel free to use more cells for this, you probably need them></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "978a37df73f74a92b6771ec2b219500c5d24072bb2710c7c15688cbcc4259e5f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
