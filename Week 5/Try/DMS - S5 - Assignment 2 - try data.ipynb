{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**this is a template notebook for Assignment 2 on Clustering. To get a 60 you will need to complete chapter 1 and 2.\n",
    "    The template is also just an indication. You can add more cells if needed, and can of course delete this line**\n",
    "\n",
    "# <span>Classification Assignment: Australia Weather<Title of your notebook>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <span>Stephen Pangga </span><br>\n",
    "Student number: <span> 629860 </span><br>\n",
    "Date: <span> 21/05/2022 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook to work you must have installed the following packages (usually via pip install *packageName*:\n",
    "* numpy\n",
    "* pandas\n",
    "* **\\<add other packages\\>**\n",
    "\n",
    "From these we will need the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy already installed, only imported\n",
      "PyPlot already installed, only imported\n",
      "pandas already installed, only imported\n",
      "seaborn already installed, only imported\n",
      "sklearn already installed, only imported\n"
     ]
    }
   ],
   "source": [
    "# enter here all those 'from .... import ....'\n",
    "# numpy as np\n",
    "try:\n",
    "    import numpy as np\n",
    "    print('NumPy already installed, only imported')\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    print('NumPy was not installed, installed and imported')\n",
    "      \n",
    "# pyplot as plt\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot already installed, only imported')\n",
    "except:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot was not installed, installed and imported')\n",
    "\n",
    "# pandas as pd   \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print('pandas already installed, only imported')\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    print('pandas was not installed, installed and imported')\n",
    "    \n",
    "try:\n",
    "    import seaborn as sn\n",
    "    print('seaborn already installed, only imported')\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sn\n",
    "    print('seaborn was not installed, installed and imported')\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print('sklearn already installed, only imported')\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print('sklearn was not installed, installed and imported')\n",
    "    \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Data\n",
    "We are going to use the datafile **<span style ='background:yellow'>weatherAUS.csv</span>**. This contains data **<span style ='background:yellow'> from kaggle and its about the daily weather for the past 10 years from the country Australia </span>**.\n",
    "\n",
    "\n",
    "\n",
    "The explanation of the variable and type from https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the code to load the data\n",
    "data = pd.read_csv('weatherAUSv4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity9am</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>17.9</td>\n",
       "      <td>35.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.3</td>\n",
       "      <td>SSW</td>\n",
       "      <td>48.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>1004.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>33.4</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>S</td>\n",
       "      <td>37.0</td>\n",
       "      <td>SSE</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1012.9</td>\n",
       "      <td>1012.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>19.4</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>NNE</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.3</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>34.9</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>21.9</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>12.2</td>\n",
       "      <td>WNW</td>\n",
       "      <td>31.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1012.7</td>\n",
       "      <td>1009.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.1</td>\n",
       "      <td>35.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>24.2</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.4</td>\n",
       "      <td>WNW</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1010.7</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>37.6</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "6049  2009-01-01    Cobar     17.9     35.2       0.0         12.0      12.3   \n",
       "6050  2009-01-02    Cobar     18.4     28.9       0.0         14.8      13.0   \n",
       "6052  2009-01-04    Cobar     19.4     37.6       0.0         10.8      10.6   \n",
       "6053  2009-01-05    Cobar     21.9     38.4       0.0         11.4      12.2   \n",
       "6054  2009-01-06    Cobar     24.2     41.0       0.0         11.2       8.4   \n",
       "\n",
       "     WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
       "6049         SSW           48.0        ENE  ...        20.0         13.0   \n",
       "6050           S           37.0        SSE  ...        30.0          8.0   \n",
       "6052         NNE           46.0        NNE  ...        42.0         22.0   \n",
       "6053         WNW           31.0        WNW  ...        37.0         22.0   \n",
       "6054         WNW           35.0         NW  ...        19.0         15.0   \n",
       "\n",
       "      Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  \\\n",
       "6049       1006.3       1004.4       2.0       5.0     26.6     33.4   \n",
       "6050       1012.9       1012.1       1.0       1.0     20.3     27.0   \n",
       "6052       1012.3       1009.2       1.0       6.0     28.7     34.9   \n",
       "6053       1012.7       1009.1       1.0       5.0     29.1     35.6   \n",
       "6054       1010.7       1007.4       1.0       6.0     33.6     37.6   \n",
       "\n",
       "      RainToday  RainTomorrow  \n",
       "6049         No            No  \n",
       "6050         No            No  \n",
       "6052         No            No  \n",
       "6053         No            No  \n",
       "6054         No            No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enter the code to see the first few rows of the data\n",
    "#remove na values\n",
    "data.dropna(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the variables (/fields/columns) you are going to use for your prediction. The dependent y (the one you are going to predict) and two independent x's (the ones you are going to use to predict that y). What do these variables mean?\\></span>**\n",
    "*note:* Its easiest if your independent x variables are numeric.\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The variabes that ill be using are Rainfall, RainToday and RainTomorrow are going to be use for my prediction.\n",
    "The Y dependent will be the RainTomorrow, the X indepentedent variables that i will be using are RainFall and RainToday.\n",
    "The x variables:\n",
    "* Rainfall - is the amount of rainfall that was recorded for the day. Measured in mm.\n",
    "* RainToday - is the representation if it rain today or not. The data is a boolean, 1 (yes) if the precipitation in the last 24 hours has exceeded 1mm if not then 0 (no).\n",
    "The y variables:\n",
    "* RainTomorrow: is the amount of rain the next day in mm. A kind of measurement or the posibility of rain the next day presented in boolean values.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical dependent variable <span style ='background:blue'> RainTomorrow <your y variable name></span> has the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['No', 'Yes'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code that returns the different categories in the y variable.\n",
    "data['RainTomorrow'] = pd.Categorical(data['RainTomorrow'])\n",
    "data['RainTomorrow'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6054</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rainfall  RainToday  RainTomorrow\n",
       "6049       0.0          0             0\n",
       "6050       0.0          0             0\n",
       "6052       0.0          0             0\n",
       "6053       0.0          0             0\n",
       "6054       0.0          0             0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert yes or no to 1 or 2 as it will be easier to handle\n",
    "data['RainToday'] = pd.Categorical(data['RainToday'])\n",
    "\n",
    "#to make it easier to visualize the data make a new dataframe with the column i will be using\n",
    "weather_df = pd.DataFrame()\n",
    "\n",
    "#add to the dataframe\n",
    "weather_df['Rainfall'] = data['Rainfall']\n",
    "weather_df['RainToday'] = data['RainToday'].cat.codes\n",
    "weather_df['RainTomorrow'] = data['RainTomorrow'].cat.codes\n",
    "\n",
    "#a small visualization of the new dataframe\n",
    "weather_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need some training and testing data, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split data in training and testing\n",
    "\n",
    "#set the variables\n",
    "x_var = weather_df[['Rainfall', 'RainToday']]\n",
    "y_var = weather_df['RainTomorrow']\n",
    "\n",
    "#split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_var, y_var, test_size=0.2, random_state=109)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set, lets try to predict this using our independent variables **<span> Rainfall & RainToday.<your x variables names></span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Basic Classification Models\n",
    "\n",
    "In the Jupyter Notebook from lecture 5 a few different Clustering techniques were discussed. Lets explore how these perform on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at some 'real' models, its a good idea to get a baseline in by using one or more of the dummy classifiers. Lets see how they perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classsifier - Most Frequent\n",
      "The accuracy score:  0.7661054172767203\n",
      "[[2093    0]\n",
      " [ 639    0]]\n",
      "Dummy classsifier - Uniform\n",
      "The accuracy score:  0.5036603221083455\n",
      "[[1086 1007]\n",
      " [ 349  290]]\n",
      "Dummy classsifier - Stratified\n",
      "The accuracy score:  0.6497071742313324\n",
      "[[1628  465]\n",
      " [ 492  147]]\n",
      "Dummy classsifier - Prior\n",
      "The accuracy score:  0.7661054172767203\n",
      "[[2093    0]\n",
      " [ 639    0]]\n"
     ]
    }
   ],
   "source": [
    "# code to create, fit and measure the dummy classifiers (see chapter 5.4. in the lecture notebook)\n",
    "# include both the accuracy score and the confusion matrix for each.\n",
    "\n",
    "# print(\"Dummy classsifier - Most Frequent\")\n",
    "# dumMF = DummyClassifier(strategy='most_frequent')\n",
    "# dumMF = dumMF.fit(x_train, y_train)\n",
    "# y_pred = dumMF.predict(x_test)\n",
    "\n",
    "def dummy_classifier (xtrain, xtest, ytrain, strats):\n",
    "    dumMF = DummyClassifier(strategy=strats)\n",
    "    dumMF = dumMF.fit(xtrain, ytrain)\n",
    "    y_pred = dumMF.predict(xtest)\n",
    "    print('The accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Dummy classsifier - Most Frequent\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'most_frequent')\n",
    "\n",
    "print(\"Dummy classsifier - Uniform\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'uniform')\n",
    "\n",
    "print(\"Dummy classsifier - Stratified\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'stratified')\n",
    "\n",
    "print(\"Dummy classsifier - Prior\")\n",
    "dummy_classifier(x_train, x_test, y_train, 'prior')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The number represents how each model of the different model of classifiers performs, which shows how accurate the classifiers are and below the accuracy score is the confusion matrix to display the performance of the classifiers and summarize the performance.\n",
    "Looking at the results, 'Most frequent' & 'Prior' classifier model have performed the best scoring identical percentage of 77%.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, those are our 'baseline'. A model should be able to at least outperform these.\n",
    "\n",
    "Lets dive in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Naive Bayes\n",
    "\n",
    "The first model discussed was the Naive Bayes model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a Naive Bayes method works></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "Naive Bayes is a algorithm that is used for classification, applying the Bayes theorem assuming that variables are independent of each other.\n",
    "Calculating probability:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(B|A)\\times P(A)}{P(B)}\n",
    "\\end{equation*}\n",
    "\n",
    "Using equation, we can find the probability of A based on the probability of B.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create and fit this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "catNB = CategoricalNB()\n",
    "catNB.fit(x_train, y_train)\n",
    "prediction1 = catNB.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to measure its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7448755490483162\n",
      "[[1743  350]\n",
      " [ 347  292]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(catNB.score(x_test, y_test))\n",
    "print(confusion_matrix(y_test, prediction1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The accuracy score with Naive Bayes model is 0.765, which is lower than Most Frequent and Prior model classifier but higher than Uniform and Stratified models. \n",
    "Both Most Frequent and Prior scores is 0.788.\n",
    "Looking at the confusion matrix\n",
    "The top left value of 7520, indicates that Naive Bayes model predicted that it will not rain tomorrow.\n",
    "The bottom right value of 1114, show how many times that will will rain the next day.\n",
    "Comparing this to one of the higher dummy classifier, the Most Frequent has the top left value of 8897 which is more than the Naive Bayes model.\n",
    "\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also have a look at what a prediction would be. If the **<span style ='background:blue'> Rainfall <your first x variable><span>** has a score of **<span style ='background:blue'>9.2<enter some value></span>** and the **<span style ='background:blue'>RainToday<your other x variable></span>** has a score of **<span style ='background:blue'>1<enter some value></span>**, then this model will predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# code to show the prediction\n",
    "# 2009-03-02,Cobar,20,31.1,0,10.4,3,SE,30,E,NW,7,6,31,31,1013.4,1012.7,7,6,23.1,29.3,No,No\n",
    "# 2009-03-13,Cobar,18.5,29.6,9.2,7.6,6,NE,39,ENE,NE,13,6,86,43,1016.1,1013.3,5,6,20,28.8,Yes,Yes\n",
    "# rainfall = 9.2\n",
    "# raintoday = yes\n",
    "# rain tomorrow = yes\n",
    "\n",
    "# lets create a new dataframe for this specific prediction\n",
    "predict_test = pd.DataFrame(columns=[\"Rainfall\", \"RainToday\", \"RainTomorrow\"]) \n",
    "predict_test.loc[0] = [9.2, 1,1]\n",
    "\n",
    "\n",
    "predictions = predict_test[[\"Rainfall\", \"RainToday\"]]\n",
    "print(catNB.predict(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style ='background:blue'> \n",
    "Which make sense as rainfall value of 9.2mm exceeded 1mm value which is minumum amount of precipitation in order to rain.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about it for NB. A nice thing about NB is that it doesn't really require any parameters. Lets look at our next technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Support Vector Machines\n",
    "The second model discussed were Support Vector Machines. There is a plural here, because we can use different kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a SVM method works></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "Support Vector Machines model predicts the dependent variable.\n",
    "The SVM algorithm plots each of the data in an x-y axis and then performs classification by placing a line between cluster of poinst to differentiates the two possible gorup of classes. which can be then used to compare to predict the dependent variables using the groups.\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic kernel is the linear one, so we'll attempt that first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the model, and fit the data.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svmLin = SVC(kernel = 'linear')\n",
    "svmLin.fit(x_train, y_train)\n",
    "y_pred = svmLin.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring its performance...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7774524158125915\n",
      "[[2073   20]\n",
      " [ 588   51]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, and the NB?></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The SVM model has produce the accuracy of 79% which is higher than the other models, Naive Bayes (76.5%) and Dummy Classifiers(Most frequent & Prior) (78.8%).\n",
    "Comparing the confusion matrix:\n",
    "it seems like SVM have 8795 times predicted that it will not rain tomorrow, which is less than the Dummy Classifier(Most frequent) - 8897 times and more than Naive Bayes - 7520 times.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the same for the other kernels that were discussed, i.e. rbf, polynomial, and sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "\n",
    "#method to create model of support vector machine and show result\n",
    "def SVM (xtrain, ytrain, xtest, ytest, kernel_type):\n",
    "    svm = SVC(kernel = kernel_type)\n",
    "    svm.fit(xtrain,ytrain)\n",
    "    y_pred = svm.predict(xtest)\n",
    "    print(metrics.accuracy_score(ytest, y_pred))\n",
    "    print(confusion_matrix(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM - RBF \n",
      "0.781112737920937\n",
      "[[2029   64]\n",
      " [ 534  105]]\n"
     ]
    }
   ],
   "source": [
    "print(' SVM - RBF ')\n",
    "SVM(x_train, y_train, x_test, y_test, 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM - Sigmoid \n",
      "0.7013177159590044\n",
      "[[1694  399]\n",
      " [ 417  222]]\n"
     ]
    }
   ],
   "source": [
    "print(' SVM - Sigmoid ')\n",
    "SVM(x_train, y_train, x_test, y_test, 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(' SVM - Polynomial ')\n",
    "# SVM(x_train, y_train, x_test, y_test, 'poly')\n",
    "\n",
    "#this SVM method took so long to give result, i ran it over night but it was not succesful\n",
    "#i ran it on google collab and got this results\n",
    "# 0.77717391304343783   - 0.7853260869565217\n",
    "# [[271  1]   - [[275  3]\n",
    "#  [ 81 15]]  - [76  14]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "\n",
    "* Linear: 0.795\n",
    "* RBF: 0.799\n",
    "* Sigmoid: 0.720\n",
    "* Polynomial: 0.777\n",
    "\n",
    "RBF and Linear have performed well compare to the rest of the SVM models, but being more precise RBF has a slight edge over Linear which makes it the best among the rest.\n",
    "Comparing the best model of SVM and Dummy classifier, RBF has a better accuracy score than Most frequent (0.788).\n",
    "\n",
    "The confusion matrix has shown that RBF have the 1930 False negative result which is better than 2387 times from the Most frequent Dummy classifier\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, lets move on to the third technique..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. K-Nearest Neighbors\n",
    "The third technique is the K-Nearest Neighbors (KNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a KNN method works></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "KNN tries to predict the correct class for the variables by calculaing the mean distance between the data points. The Selected K number of points is the closest to the test data. The algorithm calculates the possibility of test data being part of that class of the selected point('K').\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this we need to do some additional steps.\n",
    "\n",
    "First we need to normalize our x variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the StandardScaler to normalize the two x variables\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#set the scalar\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "# Convert the train and test X values, using the same scaler (so based on the X_train)\n",
    "X_trainScaled = scaler.transform(x_train)\n",
    "X_testScaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second we need to determine how many neighbors (k) we want. To do this we'll visualize the results using different values for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the graph with Error Rate vs. K-values.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate=[]\n",
    "for i in range(1,80):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_trainScaled, y_train)\n",
    "    pred_i = knn.predict(X_testScaled)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Error Rate')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The visualisation\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,80),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the result and your choice for k based on the graph></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The graph shows the performance of KNN within the range of 1 to 80. Looking at the graph, the lowest error rate value is at k = 73 which makes it the best k-value as after that there is no more lower value that that.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Note:* +0.5 if you also use the GridSearch technique to decide on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best leaf_size: 20\n",
      "Best p: 1\n",
      "Best n_neighbors: 24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_trainScaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best leaf_size:', best_model.best_estimator_.get_params()['leaf_size'])\n",
    "print('Best p:', best_model.best_estimator_.get_params()['p'])\n",
    "print('Best n_neighbors:', best_model.best_estimator_.get_params()['n_neighbors'])\n",
    "\n",
    "\n",
    "# I ran this code and took a long time, therefore i will just show the result here again.\n",
    "\n",
    "# Best leaf_size : 2\n",
    "# Best p: 1\n",
    "# Best n_neighbors: 21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch Technique took sometime to give results and didn't work with my personal laptop and i had to use google collab, i had to run it all night. So, to make it easier, I commented out the code and give the result here\n",
    "* Best leaf_size : 20\n",
    "* Best p: 1\n",
    "* Best n_neighbors: 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what we want k to be, we can create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Grid Search\n",
      "accuracy score:  0.7789165446559297\n",
      "[[2022   71]\n",
      " [ 533  106]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the model with the selected k\n",
    "knnOptimal = KNeighborsClassifier(n_neighbors=15, leaf_size=2, p=1)\n",
    "y_pred_optimal = knn.predict(X_testScaled)\n",
    "print('using Grid Search')\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred_optimal))\n",
    "print(confusion_matrix(y_test, y_pred_optimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.777086383601757\n"
     ]
    }
   ],
   "source": [
    "# code to create the model with the selected k\n",
    "knn = KNeighborsClassifier(n_neighbors=49)\n",
    "knn.fit(X_trainScaled, y_train)\n",
    "y_pred=knn.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find out how good it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k value from looking at the graph which was k=73\n",
      "accuracy score:  0.777086383601757\n",
      "[[2009   84]\n",
      " [ 525  114]]\n"
     ]
    }
   ],
   "source": [
    "# code to show its accuracy score AND confusion matrix.\n",
    "print('k value from looking at the graph which was k=73')\n",
    "print('accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, and the SVM kernels?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The KNN accuracy is 79.8%. The KNN accuracy is higher than all the dummy classifier models and Naive Bayes, although this accuracy is pretty close to SVM - RBF accuracy of 79.9% with a difference of 0.1% but if rounded up to 80% i would say they are on par of each other.\n",
    "Comparing the confusion matrix between KNN and SVM - RBF, they almost have the same confusion matrix with a little difference. The KNN predicted no rain tomorrow 1936 times while RBF predicted no rain for tomorrow 1273 times.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more basic technique to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Decision Trees\n",
    "The last technique that was discussed in detail, were the Decision Trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain briefly in your own words how a Decision Tree method works></span>**\n",
    "\n",
    "<span style ='background:blue'>\n",
    "Decision tree method predicts the dependent variable by using separated data points from the training data sets. The model then uses this data to make a series of different decision based on the coordinates of the data points which then can be used to predict the data points.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variations were discussed:\n",
    "\n",
    "* ID3 (or entropy with sklearn)\n",
    "* Gini\n",
    "* Random Forest\n",
    "* Extra trees\n",
    "\n",
    "Hopefully we have the hang of this now, so lets do each of them in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - ID3/Entropy\n",
      "0.7697657393850659\n",
      "[[2033   60]\n",
      " [ 569   70]]\n",
      "Decision Tree - Gini\n",
      "0.7697657393850659\n",
      "[[2033   60]\n",
      " [ 569   70]]\n",
      "Random Forest\n",
      "0.7719619326500732\n",
      "[[2014   79]\n",
      " [ 544   95]]\n",
      "Extra Trees\n",
      "0.7693997071742313\n",
      "[[2031   62]\n",
      " [ 568   71]]\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score AND confusion matrix.\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "print('Decision Tree - ID3/Entropy')\n",
    "ent_dtc = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "ent_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = ent_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Decision Tree - Gini')\n",
    "\n",
    "gini_dtc = DecisionTreeClassifier(criterion = \"gini\")\n",
    "gini_dtc.fit(X_trainScaled,y_train)\n",
    "y_pred = gini_dtc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Random Forest')\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfcModel = rfc.fit(X_trainScaled, y_train)\n",
    "y_pred = rfcModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Extra Trees')\n",
    "# Extremly Random Forest (a.k.a. Extra trees)\n",
    "erfc = ExtraTreesClassifier(random_state=0)\n",
    "erfc = erfc.fit(X_trainScaled, y_train)\n",
    "y_pred = erfc.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain all the results. What do the numbers mean? How is this compared to the dummy classifiers, the NB, the SVM kernels, and the knn?></span>**\n",
    "\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The accuracy of all the Decision tree classifiers are all close to each other, ID3, Gini & Extra trees have the accuracy of 79.4% while Random forest has 79.5% making it the best out of the four classifiers.\n",
    "Comparing to the other models, it performed quite well better than the dummy classifiers, NaÃ¯ve bayes but on par as SVM-RBF and KNN when it comes to their accuracy.\n",
    "The confusion matrix of random forest has shown almost similar case as KNN and SVM -RBF where it predicted that itâ€™s not going to rain tomorrow 1993 times where it is close to number of times for the other models KNN(1936 times) & SVM-RBF(1930x).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last set of techniques to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Other Models\n",
    "In the Jupyter Notebook from the lecture, in chapter 5.4 a few more techniques were simply shown:\n",
    "\n",
    "* Linear Discriminant Analysis\n",
    "* Quadratic Discriminant Analysis\n",
    "* Logistic Regression Classifier\n",
    "* Multinomial Logistic Regression Classification\n",
    "* Adaptive Boosting\n",
    "* Gradient Boosting\n",
    "* Histogram Gradient Boosting\n",
    "* XGBoost\n",
    "* Stacking\n",
    "\n",
    "Out of curiousity lets see how these perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis\n",
      "0.781112737920937\n",
      "[[2029   64]\n",
      " [ 534  105]]\n",
      "Quadratic Discriminant Analysis\n",
      "0.781112737920937\n",
      "[[2029   64]\n",
      " [ 534  105]]\n",
      "Logistic Regression Classifier\n",
      "0.781112737920937\n",
      "[[2056   37]\n",
      " [ 561   78]]\n",
      "Multinomial Logistic Regression Classification\n",
      "0.781112737920937\n",
      "[[2056   37]\n",
      " [ 561   78]]\n",
      "Adaptive Boosting\n",
      "0.7818448023426061\n",
      "[[2007   86]\n",
      " [ 510  129]]\n",
      "Gradient Boosting\n",
      "0.7781844802342606\n",
      "[[2012   81]\n",
      " [ 525  114]]\n",
      "Histogram Gradient Boosting\n",
      "0.777818448023426\n",
      "[[2037   56]\n",
      " [ 551   88]]\n",
      "Stacking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.780380673499268\n",
      "[[2045   48]\n",
      " [ 552   87]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\steph\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# code to create the models, fit the data, and show its accuracy score (the confusion matrix is here optional).\n",
    "# make sure to print some text between to indicate which result belongs to which model.\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "print('Linear Discriminant Analysis')\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "ldaModel=lda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "print('Quadratic Discriminant Analysis')\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qdaModel=qda.fit(X_trainScaled, y_train)\n",
    "y_pred=ldaModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Logistic Regression Classifier')\n",
    "logreg = LogisticRegression()\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print('Multinomial Logistic Regression Classification')\n",
    "logreg = LogisticRegression(multi_class='multinomial')\n",
    "lrModel = logreg.fit(X_trainScaled, y_train)\n",
    "y_pred = lrModel.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "print('Adaptive Boosting')\n",
    "adaBst = AdaBoostClassifier(random_state=0)\n",
    "adaBst = adaBst.fit(X_trainScaled, y_train)\n",
    "y_pred = adaBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "print('Gradient Boosting')\n",
    "gradBst = GradientBoostingClassifier(random_state=0)\n",
    "gradBst = gradBst.fit(X_trainScaled, y_train)\n",
    "y_pred = gradBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "print('Histogram Gradient Boosting')\n",
    "histBst = HistGradientBoostingClassifier(random_state=0)\n",
    "histBst = histBst.fit(X_trainScaled, y_train)\n",
    "y_pred = histBst.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                          LinearSVC(random_state=42)))]\n",
    "\n",
    "print('Stacking')\n",
    "stackCl = StackingClassifier(estimators=estimators, final_estimator = LogisticRegression())\n",
    "stackCl.fit(X_trainScaled, y_train)\n",
    "y_pred = stackCl.predict(X_testScaled)\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# xgboost\n",
    "%pip instsll xgbboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_trainScaled, y_train)\n",
    "y_pred = xgb.predict(X_testScaled)\n",
    "\n",
    "print(\"\\nXGBoost\")\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<which performed best?></span>**\n",
    "\n",
    "\n",
    "<span style ='background:blue'>\n",
    "The result of the other models are really close, having a difference in the range of 1% to 3%. but looking at the decimals, Gradient boosting has the highest accuracy of 79.95% where we can round it up to 80%.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><center>-----Chapters 1 and 2 are required to be fully completed to get a 60, the next few chapters will give a +10 for each chapter.<br> \n",
    "    However the template is not as extensive as the previous chapters. <br>\n",
    "    You can select any chapter below the order is not fixed (you can leave the others empty)<br>\n",
    "    You don't have to use the same dataset for the chapters below. If it helps in clarification you can use another dataset, but then make sure to include it as you submit.\n",
    "    ----</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualisation\n",
    "\n",
    "With two input parameters we can actually determine visually where a model will classify a variable into which category. An overview of such plots is shown at https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "We cannot copy that code since it does a comparison. What we want is a function that takes the X and Y data as input, as well as the model to be used and then shows the decision areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Categorical Input\n",
    "With classification we have a categorical output variable, but what if we also have one or more categorical input variables.\n",
    "\n",
    "One popular technique is one-hot-encoding, but there are others.\n",
    "\n",
    "In this chapter we'll discuss **<span style ='background:yellow'>\\<your chosen technique></span>**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain the technique in detail. What does it do and how does it work></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code of using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Performance\n",
    "Some models get higher accuracy scores than others. In the Jupyter Notebook from the lecture the UFC data was used and the QDA had the highest accuracy score: 0.6747. The big question is, can it be done better? First areas to look for improvement are to simply increase the number of input variables, or tweak some parameters of some of the models, or a combination of both.\n",
    "\n",
    "In this chapter we'll give it an attempt.\n",
    "\n",
    "First we need to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load the UFC data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain your attempt, what did you do.></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that will generate an accuracy score for the outcome that is higher than 0.6747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. A New Technique\n",
    "\n",
    "Many techniques were discussed in class and the lecture Jupyter Notebook, but there are a lot more. In this chapter the \\<your chosen new technique> is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<explain in detail this new technique. Note that other students should be able to understand it from your explanation alone!></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code on using this technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style ='background:yellow'>\\<feel free to use more cells for this, you probably need them></span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "978a37df73f74a92b6771ec2b219500c5d24072bb2710c7c15688cbcc4259e5f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
